<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SynergAI">
  <meta property="og:title" content="SynergAI: A Unified System for Zero-shot 3D Reasoning and Human-Robot Alignment."/>
  <meta property="og:description" content="An LLM-based system that addresses perceptual misalignment with human-robot alignment in real-time."/>
  <meta property="og:url" content="https://synerg-ai.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/icon.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="SynergAI: A Unified System for Zero-shot 3D Reasoning and Human-Robot Alignment.">
  <meta name="twitter:description" content="An LLM-based system that addresses perceptual misalignment with human-robot alignment in real-time.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Human-Robot Alignment">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SYNERGAI</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/icon.png" style="width: 250px; height: auto;">
          <h1 class="title is-size-1 publication-title">SYNERGAI: Perception Alignment for Human-Robot Collaboration</h1>

          <div class="is-size-5 publication-authors">
              <span class="author-block">
                  <a href="https://yixchen.github.io" target="_blank">Yixin Chen</a><sup>*</sup>,
              </span>
              <span class="author-block">
                  <a href="https://guoxizhang.com" target="_blank">Guoxi Zhang</a><sup>*</sup>,
              </span>
              <span class="author-block">
                  Yaowei Zhang<sup>*</sup>,
              </span>
              <span class="author-block">
                  <a href="https://sbx126.github.io" target="_blank">Hongming Xu</a><sup>*</sup>,
              </span>
              <span class="author-block">Peiyuan Zhi,</span>
              <span class="author-block">
                  <a href="https://liqing.io" target="_blank">Qing Li</a><sup>&#x1f4e7;</sup>,
              </span>
              <span class="author-block">
                  <a href="https://siyuanhuang.com" target="_blank">Siyuan Huang</a><sup>&#x1f4e7;</sup>
              </span>
          </div>

          <div class="is-size-5 publication-authors">
              <span class="author-block">Beijing Institute for General Artificial Intelligence (BIGAI)</span>
              <span class="is-size-5 eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

                <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a> -->
        </div>
      </div>
    </div>
  </div>
</section>
                  <!-- <div class="column has-text-centered"> -->
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                
                
            <!-- </div> -->
          

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop has-text-centered">
    <div class="hero-body">
        <span class="is-size-5 eql-cntrb"> <b>TL;DR:</b> We introduce SYNERGAI, a LLM-based system that can align percepturally and collaborate with humans on 3D reasoning tasks in a zero-shot manner.</span>
        </br>
      <iframe height="486" width="864" src="https://www.youtube.com/embed/sG0SK1u1Ytg"></iframe>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
        <!-- <div class="hero-body"> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-size-2">Abstract</h2>
          <div class="is-size-5 content has-text-justified">
            <p>
              Recently, large language models (LLMs) have
              shown strong potential in facilitating human-robotic interaction
              and collaboration. However, existing LLM-based systems often
              overlook the misalignment between human and robot perceptions, which 
              hinders their effective communication and real-
              world robot deployment. To address this issue, we introduce
              SYNERGAI, a unified system designed to achieve both perceptual 
              alignment and human-robot collaboration. At its core,
              SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and
              innate representation. This enables the system to leverage LLM
              to break down complex tasks and allocate appropriate tools
              in intermediate steps to extract relevant information from the
              3DSG, modify its structure, or generate responses. Importantly,
              SYNERGAI incorporates an automatic mechanism that enables
              perceptual misalignment correction with users by updating its
              3DSG with online interaction. SYNERGAI achieves comparable
              performance with the data-driven models in ScanQA in a zero-
              shot manner. Through comprehensive experiments across 10
              real-world scenes, SYNERGAI demonstrates its effectiveness in
              establishing common ground with humans, realizing a success
              rate of 61.9% in alignment tasks. It also significantly improves
              the success rate from 3.7% to 45.68% on novel tasks by
              transferring the knowledge acquired during alignment.
            </p>
      </div>
    </div>
  </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2"> Overview </h2>
        <div class="is-size-5 content has-text-justified">
          <p>
            Leveraging 3DSG as its representation, SYNERGAI decomposes complex tasks with LLMs and takes actions with our designed 
            tools in intermediate steps. It interacts with humans through natural language and non-verbal mouse clicking to
            enhance object references, capable of facilitating human-robot collaboration and perceptual alignment by automatically 
            modifying the data stored in 3DSG.
          </p>
        </div>
        <div style="width: 100%;"></div>
          <video poster="" id="scene_representation" autoplay muted loop height="100%">
            <source src="static/videos/teaser_gif.mp4" type="video/mp4">
          </video>
        </div>
        <!-- <img id="painting_icon" width="80%" src="static/images/overview.png"> -->
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2"> System Design</h2>
        <div class="is-size-5 content has-text-justified">
          <p>
            SYNERGAI represents 3D scene with 3DSGs and leverages LLMs to respond to user inputs. It is first prompted to generate 
            a <b><i>plan</i></b>, which effectively decomposes the input task into sub-tasks to be solved in a sequential process. At each step, 
            SYNERGAI selects a <b><i>tool</i></b> as its <b><i>action</i></b> based on the <b><i>observation</i></b>, which contains the results of the previous actions. In 
            this example, the system identifies the correct object of relationship “on the blue box”, but incorrectly recognizes it 
            as a book, where <b><i>perception misalignment</i></b> happens.
          </p>
        </div>
        <img id="painting_icon" width="90%" src="static/images/design.png">
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2"> Tool Design</h2>
        <div class="is-size-5 content has-text-justified">
          <p>
            The tools developed for SYNERGAI support accessing relevant information (the top five) from 3DSG, modifying its data (the 
            following four) and generating responses to the user (the last two).
          </p>
        </div>
        <img id="painting_icon" width="90%" src="static/images/tools.jpg">
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
      <h2 class="title is-size-2 has-text-centered">Human-Robot Collaboration</h2>
        <div class="is-size-5 content has-text-justified">
          We demonstrate SYNERGAI’s capability in high-level human-robot collaboration with its zero-shot
          3D reasoning task performance, including object captioning, scene captioning, question-answering and task planning. 
          We quantitatively show that it reaches comparable performance to the data-driven methods on ScanQA benchmark in a zero-shot manner.
        </div>
      <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
              <div style="text-align: center;">
                <img src="static/images/3d_reasoning_qualitative.jpg" width="90%" alt="MY ALT TEXT"/>
              </div>
              <h2 class="is-size-5 subtitle has-text-centered">
              <b>Qualitative results for various 3D reasoning tasks.</b>
              </h2>
          </div>
          <div class="item">
              <div style="text-align: center;">
                <img src="static/images/scanqa.png" width="80%" alt="MY ALT TEXT"/>
              </div>
              <h2 class="is-size-5 subtitle has-text-centered">
              <b> Zero-Shot testing on ScanQA Datasets.</b>
              </h2>
          </div> 
      </div>
  </div>
</section>

<section class="section">
<div class="container is-max-desktop">
    <h2 class="title is-size-2 has-text-centered">Human-Robot Alignment</h2>
      <div class="is-size-5 content has-text-justified">
        We systematically assess SYNERGAI’s capability in achieving perceptual alignment with humans spanning 10 real-world scenes sourced from the ScanNet dataset.
        The tests include two phases, i.e., alignment tasks and knowledge transfer. 
      </div>
    <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
            <img src="static/images/alignment.png" alt="MY ALT TEXT"/>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b>Quantitative results of human-robot alignment.</b> “SR” denotes the success rate, “RR”
              for the rate of reasonable responses and “QR” for the query ratio of the 3DSG. Our system can
              establish common ground with humans, realizing a success rate of 61.9% in alignment tasks.
            </h2>
        </div>
        <div class="item">
            <img src="static/images/alignment_ablation.png" alt="MY ALT TEXT"/>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b> Statistics of alignment experiments.</b>
            </h2>
        </div> 
        <div class="item">
          <div style="text-align: center;">
            <img src="static/images/alignment_transfer.png" width="80%" alt="MY ALT TEXT"/>
          </div>
          <h2 class="is-size-5 subtitle has-text-centered">
          <b> Knowledge Transfer to Novel Tasks, reported in Success Rate (%) as measured by LLM.</b>This shows capability to
          transfer the acquired knowledge during alignment to novel tasks.
          </h2>
        </div> 
    </div>
</div>
</section>

<section class="section">
<div class="container is-max-desktop">
    <h2 class="title is-size-2 has-text-centered">Alignment Task List and Examples</h2>
      <!-- <div class="is-size-5 content has-text-justified">
        We systematically assess SYNERGAI’s capability in achieving perceptual alignment with humans spanning 10 real-world scenes sourced from the ScanNet dataset.
        The tests include two phases, i.e., alignment tasks and knowledge transfer. 
      </div> -->
    <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <div style="text-align: center;">
            <img src="static/images/alignment_example.png" width="80%" alt="MY ALT TEXT"/>
          </div>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b>Examples of EASY and HARD alignment tasks.</b> We devise 42 tasks to assess the human-robot alignment, which are related to perceptual concepts in form of question-answering.
            </h2>
        </div>
        <div class="item">
          <div style="text-align: center;">
            <img src="static/images/Screenshot.png" width="80%" alt="MY ALT TEXT"/>
          </div>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b> A screenshot for our interface. </b> The left part consists of the reconstructed 3D scene, the local 3DSG for the object of interest (bottom left), and object segmentation (bottom middle). 
            The user can chat with our system and also select an object by clicking it in the reconstruction panel or the 3DSG. In this example, the user marks the door.
            </h2>
        </div> 
        <div class="item">
          <div style="text-align: center;">
           <img src="static/images/Full_list_1.png" width="75%" alt="MY ALT TEXT"/>
          </div>
          <h2 class="is-size-5 subtitle has-text-centered">
          <b> Full list of alignment tasks designed for the ScanNet dataset, Part 1/2.</b>
          </h2>
        </div> 
        <div class="item">
          <div style="text-align: center;">
           <img src="static/images/Full_list_2.png" width="80%" alt="MY ALT TEXT"/>
          </div>
          <h2 class="is-size-5 subtitle has-text-centered">
          <b> Full list of alignment tasks designed for the ScanNet dataset, Part 2/2.</b>
          </h2>
        </div> 
        <div class="item">
          <div style="text-align: center;">
           <img src="static/images/novel_task.png" width="80%" alt="MY ALT TEXT"/>
          </div>
          <h2 class="is-size-5 subtitle has-text-centered">
          <b> The novel tasks designed to test if the knowledge acquired during alignment is transferable.</b>
          </h2>
        </div> 
    </div>
</div>
</section>

<section class="section">
<div class="container is-max-desktop">
    <h2 class="title is-size-2 has-text-centered">Scene Reconstruction and 3DSG</h2>
      <div class="is-size-5 content has-text-justified">
        From a sequence of posed RGBD images, the 3D mesh of a scene can be reconstructed through either depth fusion and marching cubes algorithm,  or point accumulation from the 
        depth image like ConceptGraphs or via neural rendering with state-of-the-art methods like MonoSDF. We construct a 3D Scene Graph (3DSG)as its data structure, which encapsulates 
        hierarchical topology and key information necessary for 3D reasoning.
      </div>
    <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <div style="text-align: center;">
            <img src="static/images/reconstruction.png" width="75%" alt="MY ALT TEXT"/>
          </div>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b>Qualitative results of 3D reconstruction and segmentation.</b> The figure shows that different methods reveal limitations and failures in both reconstruction or segmentation.
            </h2>
        </div>
        <div class="item">
          <div style="text-align: center;">
            <img src="static/images/sg_relation.png" width="65%" alt="MY ALT TEXT"/>
          </div>
            <h2 class="is-size-5 subtitle has-text-centered">
            <b> Relationships captured in 3DSG.</b> The 3DSG is defined as a hierarchical graph, where each node represents one 3D object and the edges 
            represent spatial relationships between nodes. We instantiate the nodes with the instance segmentation and traverse all the nodes to determine their 
            spatial relationships. The object’s visual and physical properties are obtained through BLIP2 to generate information about the object color, shape, material and affordance, etc
            </h2>
        </div> 
    </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
      <h2 class="title is-size-2 has-text-centered">Prompts and Doc-strings</h2>
      <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
          <div style="text-align: center;">
              <img src="static/images/prompt1.png" width="80%" alt="MY ALT TEXT"/>
              </br>
              <img src="static/images/prompt2.png" width="80%" alt="MY ALT TEXT"/>
          </div>
              <h2 class="is-size-5 subtitle has-text-centered">
              <b>The major system prompts in SYNERGAI.</b>
              </h2>
          </div>
          <div class="item">
            <div style="text-align: center;">
              <img src="static/images/docstring.png" width="80%" alt="MY ALT TEXT"/>
            </div>
              <h2 class="is-size-5 subtitle has-text-centered">
              <b>Example doc-strings for the tool functions.</b>
              </h2>
          </div>
      </div>
  </div>
</section>

                <!-- <div class="item">
                    <img src="static/images/scanqa.png" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br> <b>2. Examples of EASY and HARD alignment tasks.</b> We device 42 tasks to assess SYNERGAI's capability for human-robot alignment. The questions in the tasks are challenging in the presence of perception errors but can be resolved after human-robot alignment. In specific, in the right example the user notices from the system response that both the hanger and the
                    ironing board have incorrect labels. Thus the user checks and corrects the label of the ironing board
                    by clicking (the 2nd and 3rd user inputs). Then the user checks the items to the left of the ironing
                    board (the 4th input) and corrects the label of the hanger (the 5th and 6th inputs).
                    </h2>
                </div>  -->
                <!-- <div class="item">
                    <img src="static/images/alignment_quantitative.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br><b>3. Quantitative results for human-robot alignment experiments.</b> In our experiment, 10 human subjects are recruited to interact with our system and correct perception errors so that the system can answer the questions in the alignment tasks. Our results show that the users are quite satified with the system's responses (Interaction SR=64.87%), and the answer accuracy is improved from 4.76% to 61.9% after alignment. 
                    </h2>
                </div>  -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section hero" id="BibTeX">
    <div class="container  is-max-desktop">
      <h2 class="title is-size-2 has-text-centered">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


<section class="section hero">
    <div class="container is-max-desktop">
        <hr class="solid">
        <div class="columns has-text-centered">
            <p><a href="https://www.bigai.ai" target="_blank"><img src="static/images/bigai.png" alt="BIGAI logo" class="center-image" width=25% textalign="center"/></a></p>
        </div>
    </div>
</section>

<!-- <footer class="footer">
    <div class="imgcontainer is-max-desktop">
        <div class="columns is-centered">
          <!-- <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. -->
            <!-- <a href="https://www.bigai.ai" target="_blank"><img src="static/images/bigai_ml2.jpg" alt="BIGAI logo" class="center-image" width=100%/></a> -->
            <!-- <a href="https://www.bigai.aih" target="_blank"><img src="static/images/bigai_ml.jpg" alt="ML logo" class="center-image" width=100%/></a> -->
        <!-- </div>
        
    </div> -->
<!-- </footer> -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
