<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SynergAI">
  <meta property="og:title" content="SynergAI: A Unified System for Zero-shot 3D Reasoning and Human-Robot Alignment."/>
  <meta property="og:description" content="An LLM-based system that addresses perceptual misalignment with human-robot alignment in real-time."/>
  <meta property="og:url" content="https://synerg-ai.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/icon.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="SynergAI: A Unified System for Zero-shot 3D Reasoning and Human-Robot Alignment.">
  <meta name="twitter:description" content="An LLM-based system that addresses perceptual misalignment with human-robot alignment in real-time.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Human-Robot Alignment">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SYNERGAI</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/icon.png" style="width: 250px; height: auto;">
          <h1 class="title is-size-1 publication-title">SYNERGAI: Perception Alignment for Human-Robot Collaboration</h1>

          <div class="is-size-5 publication-authors">
              <span class="author-block">
                  <a href="https://yixchen.github.io" target="_blank">Yixin Chen</a><sup>*</sup>,
              </span>
              <span class="author-block">
                  <a href="https://guoxizhang.com" target="_blank">Guoxi Zhang</a><sup>*</sup>,
              </span>
              <span class="author-block">
                  Yaowei Zhang<sup>*</sup>,
              </span>
              <span class="author-block">
                  <a href="https://sbx126.github.io" target="_blank">Hongming Xu</a><sup>*</sup>,
              </span>
              <span class="author-block">Peiyuan Zhi,</span>
              <span class="author-block">
                  <a href="https://liqing.io" target="_blank">Qing Li</a><sup>&#x1f4e7;</sup>,
              </span>
              <span class="author-block">
                  <a href="https://siyuanhuang.com" target="_blank">Siyuan Huang</a><sup>&#x1f4e7;</sup>
              </span>
          </div>

          <div class="is-size-5 publication-authors">
              <span class="author-block">Beijing Institute for General Artificial Intelligence (BIGAI)</span>
              <span class="is-size-5 eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

                <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a> -->
        </div>
      </div>
    </div>
  </div>
</section>
                  <!-- <div class="column has-text-centered"> -->
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                
                
            <!-- </div> -->
          

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop has-text-centered">
    <div class="hero-body">
        <span class="is-size-5 eql-cntrb"> <b>TL;DR:</b> We introduce SYNERGAI, a LLM-based system that can align percepturally and collaborate with humans on 3D reasoning tasks in a zero-shot manner.</span>
        </br>
      <iframe height="486" width="864" src="https://www.youtube.com/embed/Eol_8tWI9ac"></iframe>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section"> -->
  <div class="container is-max-desktop">
        <!-- <div class="hero-body"> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-size-2">Abstract</h2>
          <div class="is-size-5 content has-text-justified">
            <p>
              Recently, large language models (LLMs) have
              shown strong potential in facilitating human-robotic interaction
              and collaboration. However, existing LLM-based systems often
              overlook the misalignment between human and robot perceptions, which 
              hinders their effective communication and real-
              world robot deployment. To address this issue, we introduce
              SYNERGAI, a unified system designed to achieve both perceptual 
              alignment and human-robot collaboration. At its core,
              SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and
              innate representation. This enables the system to leverage LLM
              to break down complex tasks and allocate appropriate tools
              in intermediate steps to extract relevant information from the
              3DSG, modify its structure, or generate responses. Importantly,
              SYNERGAI incorporates an automatic mechanism that enables
              perceptual misalignment correction with users by updating its
              3DSG with online interaction. SYNERGAI achieves comparable
              performance with the data-driven models in ScanQA in a zero-
              shot manner. Through comprehensive experiments across 10
              real-world scenes, SYNERGAI demonstrates its effectiveness in
              establishing common ground with humans, realizing a success
              rate of 61.9% in alignment tasks. It also significantly improves
              the success rate from 3.7% to 45.68% on novel tasks by
              transferring the knowledge acquired during alignment.
            </p>
      </div>
    </div>
  </div>
<!-- </section> -->
<!-- End paper abstract -->
<section class="section hero">
    <div container>
        <div class="container  is-max-desktop">
            <h2 class="title is-size-2 has-text-centered">Approach</h2>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <img src="static/images/teaserv2.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle ">
                    <b></br>1. An overview of SYNERGAI.</b> By representing 3D scenes as 3DSGs, SYNERGAI can perform zero-shot 3D reasoning and human-robot alignment. In particular, we introduce mouse clicking to facilitate object reference in the presence of errourneous perception.
                    </h2>
                </div>
                <div class="item">
                    <img src="static/images/agent_design.png" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                        <b></br>2. The design of SYNERGAI and an example interaction.</b> SYNERGAI takes text inputs from a user and progressively collect information from the 3DSG to generate responses, during which it operates a 3DSG through a set of APIs called <em>tools</em>. As shown in the example interaction, it generates a plan first and resolves the user input step-by-step. In this example, the system incorrectly recognizes the object of interest as a book, which indicates perception misalignment.
                    </h2>
                </div> 
                <div class="item">
                    <img src="static/images/tools.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br><b>3. The set of tools developed for SYNERGAI.</b> Theses tools support accessing objects and relations (the top five) and modifying information in 3DSGs as requested (the following four). The last two tools are for generating responses to the user.
                    </h2>
                </div> 
            </div>
        </div>
        <!-- </div><hr class="solid"> -->
    </div>
</section>

<section class="section hero">
    <div container>
        <div class="container  is-max-desktop">
            <h2 class="title is-size-2 has-text-centered">Evaluation</h2>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <img src="static/images/3d_reasoning_qualitative.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br> <b>1. Qualitative results for 3D reasoning tasks.</b> SYNERGAI can solve multiple 3D reasoning tasks with a unified design.
                    </h2>
                </div>
                <div class="item">
                    <img src="static/images/alignment_task_example.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br> <b>2. Examples of EASY and HARD alignment tasks.</b> We device 42 tasks to assess SYNERGAI's capability for human-robot alignment. The questions in the tasks are challenging in the presence of perception errors but can be resolved after human-robot alignment. In specific, in the right example the user notices from the system response that both the hanger and the
                    ironing board have incorrect labels. Thus the user checks and corrects the label of the ironing board
                    by clicking (the 2nd and 3rd user inputs). Then the user checks the items to the left of the ironing
                    board (the 4th input) and corrects the label of the hanger (the 5th and 6th inputs).
                    </h2>
                </div> 
                <div class="item">
                    <img src="static/images/alignment_quantitative.jpg" alt="MY ALT TEXT"/>
                    <h2 class="is-size-5 subtitle">
                    </br><b>3. Quantitative results for human-robot alignment experiments.</b> In our experiment, 10 human subjects are recruited to interact with our system and correct perception errors so that the system can answer the questions in the alignment tasks. Our results show that the users are quite satified with the system's responses (Interaction SR=64.87%), and the answer accuracy is improved from 4.76% to 61.9% after alignment. 
                    </h2>
                </div> 
            </div>
        </div>
    </div>
</section>


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section hero" id="BibTeX">
    <div class="container  is-max-desktop">
      <h2 class="title is-size-2 has-text-centered">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


<section class="section hero">
    <div class="container is-max-desktop">
        <hr class="solid">
        <div class="columns has-text-centered">
            <p><a href="https://www.bigai.ai" target="_blank"><img src="static/images/bigai.png" alt="BIGAI logo" class="center-image" width=25% textalign="center"/></a></p>
        </div>
    </div>
</section>

<!-- <footer class="footer">
    <div class="imgcontainer is-max-desktop">
        <div class="columns is-centered">
          <!-- <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. -->
            <!-- <a href="https://www.bigai.ai" target="_blank"><img src="static/images/bigai_ml2.jpg" alt="BIGAI logo" class="center-image" width=100%/></a> -->
            <!-- <a href="https://www.bigai.aih" target="_blank"><img src="static/images/bigai_ml.jpg" alt="ML logo" class="center-image" width=100%/></a> -->
        <!-- </div>
        
    </div> -->
<!-- </footer> -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
